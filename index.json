[{"authors":["admin"],"categories":null,"content":"Dr. Peike Li is a Research Scientist at Google Research. He holds a Ph.D. from the Australian Artificial Intelligence Institute (AAII) at the University of Technology Sydney (UTS). With over 6 years of research experience, he has made significant contributions to the fields of data-efficient machine learning and multimodal generative models. His education journey also includes earning Ms.Eng and B.Eng. degrees from the renowned Shanghai Jiao Tong University. Dr. Li has an impressive track record of publishing over 15 top-tier papers in esteemed conferences and journals, including TPAMI, NeurIPS, CVPR, ICCV, and ACMMM. Dr. Li‚Äôs research expertise is centered around multi-modal generative models and large-scale pre-training models. He specializes in AIGC technology with a particular focus on transformer-based and diffusion-based multimodal generative foundation models. Dr. Li is eligible to work freely in United States üá∫üá∏ and ANZ üá¶üá∫üá≥üáø. Feel free to connect if you\u0026rsquo;re interested in exploring potential collaboration opportunities in both the academic and industry sectors.\n To accelerate the advent of machine intelligence and sustainable future\n ","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1722999758,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://peikeli.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Dr. Peike Li is a Research Scientist at Google Research. He holds a Ph.D. from the Australian Artificial Intelligence Institute (AAII) at the University of Technology Sydney (UTS). With over 6 years of research experience, he has made significant contributions to the fields of data-efficient machine learning and multimodal generative models. His education journey also includes earning Ms.Eng and B.Eng. degrees from the renowned Shanghai Jiao Tong University. Dr. Li has an impressive track record of publishing over 15 top-tier papers in esteemed conferences and journals, including TPAMI, NeurIPS, CVPR, ICCV, and ACMMM.","tags":null,"title":"Peike Li ‚òÆÔ∏è","type":"authors"},{"authors":["Peike Li","Boyu Chen","Yao Yao","Yikai Wang","Allen Wang","Alex Wang"],"categories":[],"content":"","date":1709269439,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718617373,"objectID":"4d2127ba5a90f5ebc8e7d5660ae39df8","permalink":"https://peikeli.github.io/publication/jen-1-text-guided-universal-music-generation-with-omnidirectional-diffusion-models/","publishdate":"2024-03-01T15:03:59+10:00","relpermalink":"/publication/jen-1-text-guided-universal-music-generation-with-omnidirectional-diffusion-models/","section":"publication","summary":" Music generation has attracted growing interest with the advancement of deep generative models. However, generating music conditioned on textual descriptions, known as text-to-music, remains challenging due to the complexity of musical structures and high sampling rate requirements. Despite the task‚Äôs significance, prevailing generative models exhibit limitations in music quality, computational efficiency, and generalization. This paper introduces JEN-1, a universal high-fidelity model for text-to-music generation. JEN-1 is a diffusion model incorporating both autoregressive and non-autoregressive training. Through incontext learning, JEN-1 performs various generation tasks including text-guided music generation, music inpainting, and continuation. Evaluations demonstrate JEN-1‚Äôs superior performance over state-of-the-art methods in text-music alignment and music quality while maintaining computational efficiency. Our demos are available at https://www.futureverse.com/research/jen/demos/jen1. ","tags":[],"title":"JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models","type":"publication"},{"authors":["Chen Liu","Peike Patrick Li","Qingtao Yu","Hongwei Sheng","Dadong Wang","Lincheng Li","Xin Yu"],"categories":[],"content":"","date":1706763839,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718617562,"objectID":"0a81594bd1c4b175b860b1ae6e465716","permalink":"https://peikeli.github.io/publication/benchmarking-audio-visual-segmentation-for-long-untrimmed-videos/","publishdate":"2024-02-01T15:03:59+10:00","relpermalink":"/publication/benchmarking-audio-visual-segmentation-for-long-untrimmed-videos/","section":"publication","summary":" Existing audio-visual segmentation datasets typically focus on short-trimmed videos with only one pixel-map annotation for a per-second video clip. In contrast, for untrimmed videos, the sound duration, start- and endsounding time positions, and visual deformation of audible objects vary significantly. Therefore, we observed that current AVS models trained on trimmed videos might struggle to segment sounding objects in long videos. To investigate the feasibility of grounding audible objects in videos along both temporal and spatial dimensions, we introduce the Long-Untrimmed Audio-Visual Segmentation dataset (LU-AVS), which includes precise frame-level annotations of sounding emission times and provides exhaustive mask annotations for all frames. Considering that pixel-level annotations are difficult to achieve in some complex scenes, we also provide the bounding boxes to indicate the sounding regions. Specifically, LU-AVS contains 10M mask annotations across 6.6K videos, and 11M bounding box annotations across 7K videos. Compared with the existing datasets, LUAVS videos are on average 4‚àº8 times longer, with the silent duration being 3‚àº15 times greater. Furthermore, we try our best to adapt some baseline models that were originally designed for audio-visual-relevant tasks to examine the challenges of our newly curated LU-AVS. Through comprehensive evaluation, we demonstrate the challenges of LU-AVS compared to the ones containing trimmed videos. Therefore, LU-AVS provides an ideal yet challenging platform for evaluating audio-visual segmentation and localization on untrimmed long videos. The dataset is publicly available at: https://yenanliu.github.io/LU-AVS/. ","tags":[],"title":"Benchmarking Audio Visual Segmentation for Long-Untrimmed Videos","type":"publication"},{"authors":["Chen Liu","Peike Li","Xingqun Qi","Hu Zhang","Lincheng Li","Dadong Wang","Xin Yu"],"categories":[],"content":"","date":1704085439,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718617373,"objectID":"f3f69613eff2acd11655d3e6962a6ba1","permalink":"https://peikeli.github.io/publication/audio-visual-segmentation-by-exploring-cross-modal-mutual-semantics/","publishdate":"2024-01-01T15:03:59+10:00","relpermalink":"/publication/audio-visual-segmentation-by-exploring-cross-modal-mutual-semantics/","section":"publication","summary":" The audio-visual segmentation (AVS) task aims to segment sounding objects from a given video. Existing works mainly focus on fusing audio and visual features of a given video to achieve sounding object masks. However, we observed that prior arts are prone to segment a certain salient object in a video regardless of the audio information. This is because sounding objects are often the most salient ones in the AVS dataset. Thus, current AVS methods might fail to localize genuine sounding objects due to the dataset bias. In this work, we present an audio-visual instance-aware segmentation approach to overcome the dataset bias. In a nutshell, our method first localizes potential sounding objects in a video by an object segmentation network, and then associates the sounding object candidates with the given audio. We notice that an object could be a sounding object in one video but a silent one in another video. This would bring ambiguity in training our object segmentation network as only sounding objects have corresponding segmentation masks. We thus propose a silent object-aware segmentation objective to alleviate the ambiguity. Moreover, since the category information of audio is unknown, especially for multiple sounding sources, we propose to explore the audio-visual semantic correlation and then associate audio with potential objects. Specifically, we attend predicted audio category scores to potential instance masks and these scores will highlight corresponding sounding instances while suppressing inaudible ones. When we enforce the attended instance masks to resemble the ground-truth mask, we are able to establish audio-visual semantics correlation. Experimental results on the AVS benchmarks demonstrate that our method can effectively segment sounding objects without being biased to salient objects. ","tags":[],"title":"Audio-Visual Segmentation by Exploring Cross-Modal Mutual Semantics","type":"publication"},{"authors":["Xiao Pan","Peike Li","Zongxin Yang","Huiling Zhou","Chang Zhou","Hongxia Yang","Jingren Zhou","Yi Yang"],"categories":[],"content":"","date":1646111039,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718617373,"objectID":"879925aae7390981098cd011961dc680","permalink":"https://peikeli.github.io/publication/in-n-out-generative-learning-for-dense-unsupervised-video-segmentation/","publishdate":"2022-03-01T15:03:59+10:00","relpermalink":"/publication/in-n-out-generative-learning-for-dense-unsupervised-video-segmentation/","section":"publication","summary":" In this paper, we focus on the unsupervised Video Object Segmentation (VOS) task which learns visual correspondence from unlabeled videos. Different from the previous methods which are mainly based on the contrastive learning paradigm, we propose the In-aNd-Out (INO) generative learning from a purely generative perspective. Our proposed INO captures both the high-level and fine-grained semantics, composing of the in-generative and out-generative learning. Specifically, the in-generative learning recovers the corrupted parts of an image via inferring its fine-grained semantic structure, while the out-generative learning captures high-level semantics by imagining the global information of an image given only random fragments. To better discover the temporal information, we additionally force the inter-frame consistency from both feature level and affinity matrix level. Extensive experiments on DAVIS-2017 val and YouTube-VOS 2018 val show that our INO outperforms previous state-of-the-art methods by significant margins. ","tags":[],"title":"üçîIn-N-Out Generative Learning for Dense Unsupervised Video Segmentation","type":"publication"},{"authors":["Zhikang Li","Peike Li","Huiling Zhou","Shuai Bai","Chang Zhou","Hongxia Yang"],"categories":[],"content":"","date":1643691839,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678314754,"objectID":"f7fdfda747c8993bce95955f9ba6d611","permalink":"https://peikeli.github.io/publication/m6-fashion-high-fidelity-multi-modal-image-generation-and-editing/","publishdate":"2022-02-01T15:03:59+10:00","relpermalink":"/publication/m6-fashion-high-fidelity-multi-modal-image-generation-and-editing/","section":"publication","summary":" The fashion industry has diverse applications in multi-modal image generation and editing. It aims to create a desired high-fidelity image with the multi-modal conditional signal as guidance. Most existing methods learn different condition guidance controls by introducing extra models or ignoring the style prior knowledge, which is difficult to handle multiple signal combinations and faces a low-fidelity problem. In this paper, we adapt both style prior knowledge and flexibility of multi-modal control into one unified two-stage framework, M6-Fashion, focusing on the practical AI-aided Fashion design. It decouples style codes in both spatial and semantic dimensions to guarantee high-fidelity image generation in the first stage. M6-Fashion utilizes self-correction for the non-autoregressive generation to improve inference speed, enhance holistic consistency, and support various signal controls. Extensive experiments on a large-scale clothing dataset M2C-Fashion demonstrates superior performances on various image generation and editing tasks. M6-Fashion model serves as a highly potential AI designer for the fashion industry. ","tags":[],"title":"üëóM6-Fashion: High-Fidelity Multi-modal Image Generation and Editing","type":"publication"},{"authors":["Xiao Pan","Hao Luo","Wei Jiang","Jianming Zhang","Jianyang Gu","Peike Li"],"categories":[],"content":"","date":1643173439,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677469096,"objectID":"d304bca15262e63d4949ec8a9221af66","permalink":"https://peikeli.github.io/publication/sfgn-representing-the-sequence-with-one-super-frame/","publishdate":"2022-01-26T15:03:59+10:00","relpermalink":"/publication/sfgn-representing-the-sequence-with-one-super-frame/","section":"publication","summary":"Video-based person re-identification  (V-Re-ID) is more robust than image-based person re-identification  (I-Re-ID) due to the additional temporal information. However, the  high storage overhead of video sequences largely stems the applications of V-Re-ID. To reduce the storage overhead, we propose to represent each video sequence with only one frame. However, directly picking one frame from each sequence will dramatically decrease the performance. Thus, we propose a brand-new framework called super frame generation network (SFGN) which can encode the spatial-temporal information of a video sequence into a generated frame, which is called super frame to distinguish from the directly picked key frame. To achieve super frames of high visual quality and representation ability, we carefully design the specific-frame-feature fused skip-connection generator (SFSG). SFSG takes the role of a feature encoder and the co-trained image model can be seen as the corresponding feature decoder. To reduce the information loss in the encoding-decoding process, we further propose the feature recovery loss (FRL). To the best of our knowledge, we are the first to propose and relieve this issue. Extensive experiments on Mars, iLIDS-VID and PRID2011 show that the proposed SFGN can generate super frames of high visual quality and representation ability. The code will be released after the review.","tags":[],"title":"SFGN: Representing the Sequence with One Super Frame","type":"publication"},{"authors":["Peike Li","Hongxia Yang","Yi Yang"],"categories":[],"content":"","date":1637125439,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638240936,"objectID":"242c7cab560b0ece7fd7c0db4bd74cf0","permalink":"https://peikeli.github.io/publication/open-vocabulary-panoptic-segmentation-via-multi-modal-meta-knowledge-transferring/","publishdate":"2021-11-17T15:03:59+10:00","relpermalink":"/publication/open-vocabulary-panoptic-segmentation-via-multi-modal-meta-knowledge-transferring/","section":"publication","summary":" Conventional panoptic segmentation methods can only recognize classes from a predefined close-set of categories and require a large amount of annotated samples to learn each semantic concept. However, it is exponentially challenging to enumerate, collect, and then annotate all object categories in the real open-world scenario. To eliminate such limitations, we propose a realistic yet challenging task, namely Open-Vocabulary Panoptic Segmentation (OVPS), which can segment novel categories by exploiting only the natural language text prompts without needing any mask annotations. We propose a framework called Multi-Modal Meta-Knowledge Transferring (M3KT) to panoptically segment the novel categories with zero-shot training samples. By decoupling the grouping and recognition, M3KT effectively generates visual meta-weights for each class-agnostic object. By enforcing the similarity of visual meta-weights and textual meta-weights, M3KT gradually adapts the semantic prior knowledge from the language domain to the vision domain. In this way, M3KT learns localization and recognition ability trained on the seen base categories, and able to generalize on unseen novel categories described by the open-vocabulary text prompts. Extensive experiments demonstrate the effectiveness of the proposed M3KT network evaluated on COCO-Panoptic benchmark. Our proposed M3KT also shows strong generalization ability on novel categories when evaluated on the cross-domain benchmarks from COCO-Panoptic dataset to Cityscapes-Panoptic dataset. ","tags":[],"title":"Open-Vocabulary Panoptic Segmentation via Multi-modal Meta-knowledge Transferring","type":"publication"},{"authors":["Peike Li","Xin Yu","Yi Yang"],"categories":[],"content":"","date":1605589439,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638241336,"objectID":"0ca670b37e7d5f1aff488e2ec976e708","permalink":"https://peikeli.github.io/publication/super-resolving-cross-domain-face-miniatures-bypeeking-at-one-shot-exemplar/","publishdate":"2020-11-17T15:03:59+10:00","relpermalink":"/publication/super-resolving-cross-domain-face-miniatures-bypeeking-at-one-shot-exemplar/","section":"publication","summary":"Conventional face super-resolution methods usually assume testing low-resolution (LR) images lie on the same domain as the training ones. Due to different lighting conditions and imaging hardware, domain gaps between training and testing images inevitably occur in many real-world scenarios. Neglecting those domain gaps would lead to inferior face super-resolution (FSR) performance. However, collecting large-scale data from a target domain to re-train FSR models is often time-consuming. Therefore, we aim to employ only few examples, ideally one-shot exemplar, to adapt an FSR model to a target domain rapidly. In this paper, we propose a domain-aware pyramid-based face super-resolution network, named DAP-FSR network, to super-resolve LR faces from a new domain by exploiting one paired high-resolution (HR) and LR exemplar in the target domain. When a target domain LR face is given, our DAP-FSR firstly employs its encoder extracts the multi-scale latent representations of the input face. Considering only one target domain example is available, we augment the target domain data by mixing the latent representations of the target domain face and source domain ones, and then feed the mixed representations to the decoder of our DAP-FSR. The decoder will generate new face images resembling the given target domain image. The generated HR faces in return are used to optimize our decoder to reduce the domain gap. By iteratively updating the latent representations and our decoder, our DAP-FSR will be adapted to the target domain, thus achieving authentic and high-quality upsampled HR faces. Extensive experiments on three newly constructed benchmarks validate the effectiveness and superior performance of our proposed DAP-FSR compared to the state-of-the-art methods. ","tags":[],"title":"Super-Resolving Cross-Domain Face Miniatures by Peeking at One-Shot Exemplar","type":"publication"},{"authors":["Peike Li","Yunqiu Xu","Yunchao Wei","Yi Yang"],"categories":[],"content":"","date":1604191593,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609397695,"objectID":"dd379af030be758c4e94411023495118","permalink":"https://peikeli.github.io/publication/self-correction-for-human-parsing/","publishdate":"2020-11-01T10:46:33+10:00","relpermalink":"/publication/self-correction-for-human-parsing/","section":"publication","summary":"Labeling pixel-level masks for fine-grained semantic segmentation tasks, e.g. human parsing, remains a challenging task. The ambiguous boundary between different semantic parts and those categories with similar appearance usually are confusing, leading to unexpected noises in ground truth masks. To tackle the problem of learning with label noises, this work introduces a purification strategy, called Self-Correction for Human Parsing (SCHP), to progressively promote the reliability of the supervised labels as well as the learned models. In particular, starting from a model trained with inaccurate annotations as initialization, we design a cyclically learning scheduler to infer more reliable pseudo-masks by iteratively aggregating the current learned model with the former optimal one in an online manner. Besides, those correspondingly corrected labels can in turn to further boost the model performance. In this way, the models and the labels will reciprocally become more robust and accurate during the self-correction learning cycles. Benefiting from the superiority of SCHP, we achieve the best performance on two popular single-person human parsing benchmarks, including LIP and Pascal-Person-Part datasets. Our overall system ranks 1st in CVPR2019 LIP Challenge. Code is available at https://github.com/PeikeLi/Self-Correction-Human-Parsing.","tags":[],"title":"Self-Correction for Human Parsing","type":"publication"},{"authors":["Peike Li","Yunchao Wei","Yi Yang"],"categories":[],"content":"","date":1593666239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616585414,"objectID":"1951c7354e5351a87d7d839df5b8cdbe","permalink":"https://peikeli.github.io/publication/consistent-structural-relation-learning-for-zero-shot-segmentation/","publishdate":"2020-07-02T15:03:59+10:00","relpermalink":"/publication/consistent-structural-relation-learning-for-zero-shot-segmentation/","section":"publication","summary":"Zero-shot semantic segmentation aims to recognize the semantics of pixels from unseen categories with zero training samples. Previous practice [1] proposed to train the classifiers for unseen categories using the visual features generated from semantic word embeddings. However, the generator is merely learned on the seen categories while no constraint is applied to the unseen categories, leading to poor generalization ability. In this work, we propose a Consistent Structural Relation Learning (CSRL) approach to constrain the generating of unseen visual features by exploiting the structural relations between seen and unseen categories. We observe that different categories are usually with similar relations in either semantic word embedding space or visual feature space. This observation motivates us to harness the similarity of category-level relations on the semantic word embedding space to learn a better visual feature generator. Concretely, by exploring the pair-wise and list-wise structures, we impose the relations of generated visual features to be consistent with their counterparts in the semantic word embedding space. In this way, the relations between seen and unseen categories will be transferred to implicitly constrain the generator to produce relation-consistent unseen visual features. We conduct extensive experiments on Pascal-VOC and Pascal-Context benchmarks. The proposed CSRL significantly outperforms existing state-of-the-art methods by a large margin, resulting in ~7-12% on Pascal-VOC and ~2-5% on Pascal-Context. ","tags":[],"title":"Consistent Structural Relation Learning for Zero-Shot Segmentation","type":"publication"},{"authors":["Peike Li","Yunchao Wei","Yi Yang"],"categories":[],"content":"","date":1589185238,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602546198,"objectID":"15905bdd07175307e828dd3ad5180ab7","permalink":"https://peikeli.github.io/publication/meta-parsing-networks-towards-open-set-scene-parsing-with-adaptive-metric-learning/","publishdate":"2020-05-11T18:20:38+10:00","relpermalink":"/publication/meta-parsing-networks-towards-open-set-scene-parsing-with-adaptive-metric-learning/","section":"publication","summary":"Recent progress in few-shot segmentation usually aims at performing novel object segmentation using a few annotated examples as guidance. In this work, we advance this few-shot segmentation paradigm towards a more challenging yet general scenario, i.e., Generalized Few-shot Scene Parsing (GFSP). In this task, we take a fully annotated image as guidance to segment all pixels in a query image. Our mission is to study a generalizable and robust segmentation network from the meta-learning perspective so that both seen and unseen categories can be correctly recognized. Different from previous practices, this task performs segmentation on a joint label space consisting of both previously seen and novel categories. Moreover, pixels from these multiple categories need to be simultaneously taken into account, which is actually not well explored before. Accordingly, we present Meta Parsing Networks (MPNet) to better exploit the guidance information in the support set. Our MPNet contains two basic modules, i.e., the Adaptive Deep Metric Learning (ADML) module and the Contrastive Inter-class Distraction (CID) module. Specially, the ADML takes the annotated pixels from the support image as the guidance and adaptively produces high-quality prototypes for learning a deep comparison metric. In addition, MPNet further introduces the CID module learning to enlarge the feature discrepancy of different categories in the embedding space, leading the MPNet to generate more discriminative feature embeddings. We conduct experiments on two newly constructed benchmarks, i.e., GFSP-Cityscapes and GFSP-Pascal-Context. Extensive ablation studies well demonstrate the effectiveness and generalization ability of our MPNet.","tags":[],"title":"Meta Parsing Networks: Towards Generalized Few-shot Scene Parsing with Adaptive Metric Learning","type":"publication"},{"authors":["Peike Li","Xuanyi Dong","Xin Yu","Yi Yang"],"categories":[],"content":"","date":1589185238,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599541485,"objectID":"76adf8d0f914a8acf1b8973830f6cabe","permalink":"https://peikeli.github.io/publication/when-humans-meet-machines-towards-efficient-segmentation-networks/","publishdate":"2020-05-11T18:20:38+10:00","relpermalink":"/publication/when-humans-meet-machines-towards-efficient-segmentation-networks/","section":"publication","summary":"In this paper, we investigate how to achieve a high-performance yet lightweight segmentation network for real-time applications. By analyzing three typical segmentation networks, we observe that the segmentation backbones and heads are often imbalanced which restricts network efficiency. Thus, we develop a lightweight context fusion (LCF) module and a lightweight global enhancement (LGE) module to construct our lightweight segmentation head. Specifically, LCF fuses multi-resolution features to capture image details and LGE is designed to enhance feature representations. In this manner, our lightweight head facilities network efficiency and significantly reduces network parameters. Furthermore, we design a Multi-Resolution Macro Segmentation structure (MRMS) to incorporate human knowledge into our network architecture composition. Given the resource-aware constraint (e.g., latency time), we optimize our network with network architecture search while considering the relationships among atomic operators, network depth and feature resolution in segmentation tasks. Since MRMS embeds the segmentation-specific knowledge, it also provides better architecture search space. Our Human-Machine collaboratively designed Segmentation network (HMSeg) achieves better performance and faster inference speed. Experiments demonstrate that our network achieves 71.4% mean intersection over union (mIOU) on Cityscapes with only 0.7M parameters at 172.4 FPS on NVIDIA GTX1080Ti.","tags":[],"title":"When Humans Meet Machines: Towards Efficient Segmentation Networks","type":"publication"},{"authors":["Qianyu Feng","Zongxin Yang","Peike Li","Yunchao Wei","Yi Yang"],"categories":[],"content":"","date":1569717993,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596075841,"objectID":"80534c5297081cdab572911b4c1bc491","permalink":"https://peikeli.github.io/publication/dual-embedding-learning-for-video-instance-segmentation/","publishdate":"2019-09-29T10:46:33+10:00","relpermalink":"/publication/dual-embedding-learning-for-video-instance-segmentation/","section":"publication","summary":"In this paper, we propose a novel framework to gener-ate high-quality segmentation results in a two-stage style,aiming at video instance segmentation task which requiressimultaneous  detection,  segmentation  and  tracking  of  in-stances. To address this multi-task efficiently, we opt to firstselect high-quality detection proposals in each frame.  Thecategories of the proposals are calibrated with the globalcontext of video.  Then, each selected proposal is extendedtemporally by a bi-directional Instance-Pixel Dual-Tracker(IPDT) which synchronizes the tracking on both instance-level  and  pixel-level.   The  instance-level  module  concen-trates on distinguishing the target instance from other ob-jects while the pixel-level module focuses more on the lo-cal feature of the instance.  Our proposed method achieveda  competitive  result  of  mAP  45.0%  on  the  Youtube-VOSdataset, ranking the 3rd in Track 2 of the 2nd Large-scaleVideo Object Segmentation Challenge.","tags":[],"title":"Dual Embedding Learning for Video Instance Segmentation","type":"publication"},{"authors":["Zongxin Yang","Peike Li","Qianyu Feng","Yunchao Wei","Yi Yang"],"categories":[],"content":"","date":1569717993,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596075841,"objectID":"6aa1b2e096fbbbfbfb67644c21294a92","permalink":"https://peikeli.github.io/publication/going-deeper-into-embedding-learning-for-video-object-segmentation/","publishdate":"2019-09-29T10:46:33+10:00","relpermalink":"/publication/going-deeper-into-embedding-learning-for-video-object-segmentation/","section":"publication","summary":"In  this  paper,  we  investigate  the  principles  of  consis-tent  training,  between  given  reference  and  predicted  se-quence,  for better embedding learning of semi-supervisedvideo object segmentation.  To accurately segment the tar-get  objects  given  the  mask  at  the  first  frame,  we  realizethat  the  expected  feature  embeddings  of  any  consecutiveframes  should  satisfy  the  following  properties: 1)globalconsistency in terms of both foreground object(s) and back-ground; 2)robust local consistency under a various objectmoving rate; 3)environment consistency between the train-ing and inference process; 4)receptive consistency betweenthe  receptive  fields  of  network  and  the  variable  scales  ofobjects;5)sampling consistency between foreground andbackground pixels to avoid training bias.  With the princi-ples in mind,  we carefully design a simple pipeline to liftboth accuracy and efficiency for video object segmentationeffectively. With the ResNet-101 as the backbone, our singlemodel achieves a J\u0026F score of 81.0% on the validation setof Youtube-VOS benchmark without any bells and whistles.By applying multi-scale \u0026 flip augmentation at the testingstage, the accuracy can be further boosted to 82.4%. Codewill be made available.","tags":[],"title":"Going Deeper into Embedding Learning for Video Object Segmentation","type":"publication"},{"authors":["Peike Li","Pingbo Pan","Ping Liu","Mingliang Xu","Yi Yang"],"categories":[],"content":"","date":1568449238,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599528559,"objectID":"a5731516d254697a48a8c6f0ce0aaeac","permalink":"https://peikeli.github.io/publication/hierarchical-temporal-modeling-with-mutual-distance-matching-for-video-based-person-re-identification/","publishdate":"2019-09-14T18:20:38+10:00","relpermalink":"/publication/hierarchical-temporal-modeling-with-mutual-distance-matching-for-video-based-person-re-identification/","section":"publication","summary":"Comparing to image-based person re-identification (re-ID) problems, video-based person re-ID can take advantage of more cues from appearance and temporal information, and therefore receives widespread attention recently. However, due to the different pose, occlusion, misalignment and multi-granularity in video sequences, those consequent inter-sequence variations and intra-sequence variations, inevitably makes the feature learning and matching in videos more difficult. Under this circumstance, it is necessary to design an effective discriminative representation learning mechanism, as well as a matching solution, to tackle these variations in video-based person re-ID. To this end, this paper introduces a multi-granularity temporal convolution network and a mutual distance matching measurement, aiming at alleviating the intra-sequence variation and the inter-sequence variation, respectively. Particularly, in the feature learning stage, we model different temporal granularities by hierarchically stacking temporal convolution blocks with different dilation factors. In the feature matching stage, we propose a clip-level probe-gallery mutual distance measurement and consider the most convincing clip pairs by top-k selection. We validate that our proposed method can achieve state-of-the-art results on three video-based person re-ID benchmarks, more than that, we conduct extensive ablation study to demonstrate conciseness and effectiveness of our method in video re-ID tasks.","tags":[],"title":"Hierarchical Temporal Modeling with Mutual Distance Matching for Video Based Person Re-Identification","type":"publication"},{"authors":null,"categories":null,"content":"Music generation has attracted growing interest with the advancement of deep generative models. However, generating music conditioned on textual descriptions, known as text-to-music, remains challenging due to the complexity of musical structures and high sampling rate requirements. Despite the task\u0026rsquo;s significance, prevailing generative models exhibit limitations in music quality, computational efficiency, and generalization. This paper introduces JEN-1, a universal high-fidelity model for text-to-music generation. JEN-1 is a diffusion model incorporating both autoregressive and non-autoregressive training. Through in-context learning, JEN-1 performs various generation tasks including text-guided music generation, music inpainting, and continuation. Evaluations demonstrate JEN-1\u0026rsquo;s superior performance over state-of-the-art methods in text-music alignment and music quality while maintaining computational efficiency. Our demos are available at https://www.futureverse.com/research/jen/demos/jen1.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692165734,"objectID":"359c045e6dbf381aa72b6e9ccb44e727","permalink":"https://peikeli.github.io/project/jen-1/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/jen-1/","section":"project","summary":"Text-Guided Universal Music Generation with Omnidirectional Diffusion Models.","tags":["Deep Learning"],"title":"JEN-1","type":"project"},{"authors":null,"categories":null,"content":"Among the 75 participating teams in the world, our team is proud to announce that we have won the Look Into Person (LIP) Challenge for three tasks, including the single-person human parsing, the multi-person human parsing and multi-person video parsing tracks. LIP challenge has attracted many world-famous AI companies and universities including the ByteDance AI Lab, JD AI Lab, Samsung AI Lab UCLA, Cambridge and so on.\nLIP Challenge is co-organized with workshop on augmented human: human-centric understanding. The workshop is co-located with CVPR 2019, the most important annual computer vision conference, to be held in Long Beach, California 16-23 June. The goal of this workshop is to allow researchers from the fields of human-centric understanding and 2D/3D synthesis to present their progress, communication and co-develop novel ideas that potentially shape the future of this area and further advance the performance and applicability of correspondingly built systems in real-world conditions.\nThe results can be found in https://vuhcs.github.io/.\n  Award Certificate    Award Certificate    Award Certificate   The third large-scale Look Into Person (LIP) challenges which include five competition tasks: the single-person human parsing, the single-person pose estimation, the multi-person human parsing, multi-person video parsing, multi-person pose estimation benchmark, and clothes virtual try-on benchmark. This third LIP challenge mainly extends the second LIP challenge in CVPR 2017 and CVPR 2018 by additionally covering a video human parsing challenge and the 2D/3D clothes virtual try-on benchmark. For the single-person human parsing and pose estimation, 50,000 images with elaborated pixel-wise annotations with comprehensive 19 semantic human part labels and 2D human poses with 16 dense key points are provided. For the multi-person human parsing competition task, another 50000 images of crowded scenes with 19 semantic human part labels are provided. For video-based human parsing, 3000 video shots with 1-2 minutes will be densely annotated with 19 semantic human part labels. For multi-person pose estimation, the dataset contains 25,828 images with 2D human poses with 16 dense key points and head \u0026amp; instance bounding boxes. The new image-based clothes try-on benchmark targets at fitting new in-shop clothes into a person image and generate one try-on video to show different clothes viewpoints on the person. The images collected from the real-world scenarios contain humans appearing with challenging poses and views, heavily occlusions, various appearances and low-resolutions. This challenge was released before January, 2019 and the challenge is conjunction with CVPR 2019, Long Beach, CA.\nHuman Parsing is the sub-task of semantic segmentation which need to assign a semantic label (like leg or arm) to each pixel in a human image. We proposed a novel SPHP(Single Person Human Parsing) network, which consists of three key modules to learn for parsing in an end-to-end manner including high resolution embedding module, global context embedding module and edge perceiving module.\n  Single Person Parsing Visualization    Multiple Person Parsing Visualization    Video Multiple Person Parsing Visualization   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601079773,"objectID":"77e35d9fb208659a988c4f9f76cb932d","permalink":"https://peikeli.github.io/competitions/cvpr2019-lip-challenge/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/competitions/cvpr2019-lip-challenge/","section":"competitions","summary":":trophy:Winner of single-person human parsing (Track1), multi-person human parsing (Track3) and multi-person video human parsing (Track4).","tags":["Competition"],"title":"CVPR2019 LIP Challenge 1st Place","type":"competitions"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601079773,"objectID":"73c43e7c32a094f1e5e7e420d4e1dec8","permalink":"https://peikeli.github.io/competitions/iccv2019-youtube-vos-challenge/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/competitions/iccv2019-youtube-vos-challenge/","section":"competitions","summary":":trophy:Semi-supervised Video Object Segmentation (Track 1) \u0026 Video Instance Segmentation Task (Track 2)","tags":["Competition"],"title":"ICCV2019 Youtube-VOS Challenge 3rd Place","type":"competitions"}]